{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: spacy in c:\\users\\sageo\\anaconda3\\lib\\site-packages (3.8.4)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in c:\\users\\sageo\\anaconda3\\lib\\site-packages (from spacy) (3.0.12)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in c:\\users\\sageo\\anaconda3\\lib\\site-packages (from spacy) (1.0.5)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in c:\\users\\sageo\\anaconda3\\lib\\site-packages (from spacy) (1.0.12)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in c:\\users\\sageo\\anaconda3\\lib\\site-packages (from spacy) (2.0.11)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in c:\\users\\sageo\\anaconda3\\lib\\site-packages (from spacy) (3.0.9)\n",
      "Requirement already satisfied: thinc<8.4.0,>=8.3.4 in c:\\users\\sageo\\anaconda3\\lib\\site-packages (from spacy) (8.3.4)\n",
      "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in c:\\users\\sageo\\anaconda3\\lib\\site-packages (from spacy) (1.1.3)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in c:\\users\\sageo\\anaconda3\\lib\\site-packages (from spacy) (2.5.1)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in c:\\users\\sageo\\anaconda3\\lib\\site-packages (from spacy) (2.0.10)\n",
      "Requirement already satisfied: weasel<0.5.0,>=0.1.0 in c:\\users\\sageo\\anaconda3\\lib\\site-packages (from spacy) (0.4.1)\n",
      "Requirement already satisfied: typer<1.0.0,>=0.3.0 in c:\\users\\sageo\\anaconda3\\lib\\site-packages (from spacy) (0.15.2)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in c:\\users\\sageo\\anaconda3\\lib\\site-packages (from spacy) (4.66.5)\n",
      "Requirement already satisfied: numpy>=1.19.0 in c:\\users\\sageo\\anaconda3\\lib\\site-packages (from spacy) (1.26.4)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in c:\\users\\sageo\\anaconda3\\lib\\site-packages (from spacy) (2.32.3)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in c:\\users\\sageo\\anaconda3\\lib\\site-packages (from spacy) (2.8.2)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\sageo\\anaconda3\\lib\\site-packages (from spacy) (3.1.4)\n",
      "Requirement already satisfied: setuptools in c:\\users\\sageo\\anaconda3\\lib\\site-packages (from spacy) (75.1.0)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\sageo\\anaconda3\\lib\\site-packages (from spacy) (24.1)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in c:\\users\\sageo\\anaconda3\\lib\\site-packages (from spacy) (3.5.0)\n",
      "Requirement already satisfied: language-data>=1.2 in c:\\users\\sageo\\anaconda3\\lib\\site-packages (from langcodes<4.0.0,>=3.2.0->spacy) (1.3.0)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in c:\\users\\sageo\\anaconda3\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.6.0)\n",
      "Requirement already satisfied: pydantic-core==2.20.1 in c:\\users\\sageo\\anaconda3\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (2.20.1)\n",
      "Requirement already satisfied: typing-extensions>=4.6.1 in c:\\users\\sageo\\anaconda3\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (4.11.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\sageo\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\sageo\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\sageo\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\sageo\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (2024.12.14)\n",
      "Requirement already satisfied: blis<1.3.0,>=1.2.0 in c:\\users\\sageo\\anaconda3\\lib\\site-packages (from thinc<8.4.0,>=8.3.4->spacy) (1.2.0)\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in c:\\users\\sageo\\anaconda3\\lib\\site-packages (from thinc<8.4.0,>=8.3.4->spacy) (0.1.5)\n",
      "Requirement already satisfied: colorama in c:\\users\\sageo\\anaconda3\\lib\\site-packages (from tqdm<5.0.0,>=4.38.0->spacy) (0.4.6)\n",
      "Requirement already satisfied: click>=8.0.0 in c:\\users\\sageo\\anaconda3\\lib\\site-packages (from typer<1.0.0,>=0.3.0->spacy) (8.1.7)\n",
      "Requirement already satisfied: shellingham>=1.3.0 in c:\\users\\sageo\\anaconda3\\lib\\site-packages (from typer<1.0.0,>=0.3.0->spacy) (1.5.4)\n",
      "Requirement already satisfied: rich>=10.11.0 in c:\\users\\sageo\\anaconda3\\lib\\site-packages (from typer<1.0.0,>=0.3.0->spacy) (13.7.1)\n",
      "Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in c:\\users\\sageo\\anaconda3\\lib\\site-packages (from weasel<0.5.0,>=0.1.0->spacy) (0.21.0)\n",
      "Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in c:\\users\\sageo\\anaconda3\\lib\\site-packages (from weasel<0.5.0,>=0.1.0->spacy) (5.2.1)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\sageo\\anaconda3\\lib\\site-packages (from jinja2->spacy) (2.1.3)\n",
      "Requirement already satisfied: marisa-trie>=1.1.0 in c:\\users\\sageo\\anaconda3\\lib\\site-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy) (1.2.1)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in c:\\users\\sageo\\anaconda3\\lib\\site-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (2.2.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in c:\\users\\sageo\\anaconda3\\lib\\site-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (2.15.1)\n",
      "Requirement already satisfied: mdurl~=0.1 in c:\\users\\sageo\\anaconda3\\lib\\site-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (0.1.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting en-core-web-sm==3.8.0\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.8.0/en_core_web_sm-3.8.0-py3-none-any.whl (12.8 MB)\n",
      "     ---------------------------------------- 0.0/12.8 MB ? eta -:--:--\n",
      "     ---------- ----------------------------- 3.4/12.8 MB 16.8 MB/s eta 0:00:01\n",
      "     --------------------------- ------------ 8.9/12.8 MB 21.3 MB/s eta 0:00:01\n",
      "     --------------------------------------- 12.8/12.8 MB 21.2 MB/s eta 0:00:00\n",
      "\u001b[38;5;2mâœ” Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('en_core_web_sm')\n"
     ]
    }
   ],
   "source": [
    "!python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 1: Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['The', 'quick', 'brown', 'fox', 'does', \"n't\", 'jump', 'over', 'the', 'lazy', 'dog', '.', 'Natural', 'Language', 'Processing', 'is', 'fascinating', '!']\n"
     ]
    }
   ],
   "source": [
    "# 1. Write a Python script to tokenize the following text: \"The quick brown fox doesn't jump over the lazy dog. Natural Language Processing is fascinating!\"\n",
    "text = (\"\"\"The quick brown fox doesn't jump over the lazy dog. Natural Language Processing is fascinating!\"\"\")\n",
    "doc = nlp(text)\n",
    "tokens_spacy = [token.text for token in doc]\n",
    "print(tokens_spacy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### How does spaCy process the various tokens?:\n",
    "SpaCy splits the text into individual tokens-- words, punctuation marks, or contraactions-- each of which has certain attributes including text (text_), base form of the word (.lemma_), gramatical features (.morph), and the word it depends on in the sentence structure (.head). \n",
    "#### How does spaCy handle punctuation marks like periods and commas?:\n",
    "SpaCy handles punctuation marks seperately, making them their own token rather than being attatched to words. \n",
    "#### What happens when the text includes contractions (e.g., \"don't\")?:\n",
    "When the text has contractions, spaCy splits them into multiple tokens. For example, \"doesn't\" is split into \"does\" and \"n't\". This is because\"n't\" is not recognized as part of the verb \"does,\" but rather a negation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2: Part-of-Speech Tagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The - the: DET\n",
      "quick - quick: ADJ\n",
      "brown - brown: ADJ\n",
      "fox - fox: NOUN\n",
      "does - do: AUX\n",
      "n't - not: PART\n",
      "jump - jump: VERB\n",
      "over - over: ADP\n",
      "the - the: DET\n",
      "lazy - lazy: ADJ\n",
      "dog - dog: NOUN\n",
      ". - .: PUNCT\n",
      "Natural - Natural: PROPN\n",
      "Language - Language: PROPN\n",
      "Processing - processing: NOUN\n",
      "is - be: AUX\n",
      "fascinating - fascinating: ADJ\n",
      "! - !: PUNCT\n"
     ]
    }
   ],
   "source": [
    "# 1. Extend your script to include part-of-speech tagging for the tokens.\n",
    "for token in doc:\n",
    "    print(f\"{token.text} - {token.lemma_}: {token.pos_}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Identify the POS tags for \"quick,\" \"jumps,\" and \"is.\"\n",
    "As demonstrated in my code, the POS tag for \"quick\" is ADJ (adjective), for \"jumps\" is VERB, and for \"is\" is AUX (auxiliary verb). \n",
    "#### Why might POS tagging be useful for tasks like grammar checking or machine translation?\n",
    "For grammar checking, POS can help identify incorrect word usage, like checking if a noun is missing a verb. For machine translation, knowing the POS of each word can help translate sentences most accurately. For example, understanding the difference between \"run\" as a verb and \"run\" as a noun. Finally for text analysis, POS can help in sentiment analysis, chatbots, and speech recognition processes by understanding sentence structure."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 3: Named Entity Recognition (NER)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Barack Obama: PERSON(People, including fictional)\n",
      "44th: ORDINAL(\"first\", \"second\", etc.)\n",
      "the United States: GPE(Countries, cities, states)\n",
      "Hawaii: GPE(Countries, cities, states)\n"
     ]
    }
   ],
   "source": [
    "# 1. Modify your script to identify named entities in the following text: \"Barack Obama was the 44th President of the United States. He was born in Hawaii.\"\n",
    "text2 = (\"\"\"Barack Obama was the 44th President of the United States. He was born in Hawaii.\"\"\")\n",
    "doc = nlp(text2)\n",
    "for ent in doc.ents:\n",
    "    print(f\"{ent.text}: {ent.label_}({spacy.explain(ent.label_)})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Which entities are recognized by spaCy?\n",
    "As shown in my code, spaCy identified Barak Obama as a person, 44th as ordinal, President as a title, United States as a GPE (geopolitical entity, according to readings), and Hawaii as a GPE as well. \n",
    "#### What entity types are assigned to \"Barack Obama\" and \"Hawaii\"?\n",
    "Barak Obama is assigned PERSON and Hawaii is assigned GPE (geopolitical entity). \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 4: Experimentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Write a new sentence or paragraph of your choice and run the spaCy pipeline on it.\n",
    "text3 = (\"\"\"My friends and I saw Deftones in concert this weekend in Indianapolis.\"\"\")\n",
    "doc = nlp(text3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text: My, POS: PRON, Lemma: my, Morph: Number=Sing|Person=1|Poss=Yes|PronType=Prs\n",
      "Text: friends, POS: NOUN, Lemma: friend, Morph: Number=Plur\n",
      "Text: and, POS: CCONJ, Lemma: and, Morph: ConjType=Cmp\n",
      "Text: I, POS: PRON, Lemma: I, Morph: Case=Nom|Number=Sing|Person=1|PronType=Prs\n",
      "Text: saw, POS: VERB, Lemma: see, Morph: Tense=Past|VerbForm=Fin\n",
      "Text: Deftones, POS: PROPN, Lemma: Deftones, Morph: Number=Sing\n",
      "Text: in, POS: ADP, Lemma: in, Morph: \n",
      "Text: concert, POS: NOUN, Lemma: concert, Morph: Number=Sing\n",
      "Text: this, POS: DET, Lemma: this, Morph: Number=Sing|PronType=Dem\n",
      "Text: weekend, POS: NOUN, Lemma: weekend, Morph: Number=Sing\n",
      "Text: in, POS: ADP, Lemma: in, Morph: \n",
      "Text: Indianapolis, POS: PROPN, Lemma: Indianapolis, Morph: Number=Sing\n",
      "Text: ., POS: PUNCT, Lemma: ., Morph: PunctType=Peri\n",
      "Entity: Deftones, Label: ORG\n",
      "Entity: this weekend, Label: DATE\n",
      "Entity: Indianapolis, Label: GPE\n"
     ]
    }
   ],
   "source": [
    "# Information about tokens and entities:\n",
    "for token in doc:\n",
    "    print(f\"Text: {token.text}, POS: {token.pos_}, Lemma: {token.lemma_}, Morph: {token.morph}\")\n",
    "\n",
    "for ent in doc.ents:\n",
    "    print(f\"Entity: {ent.text}, Label: {ent.label_}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text: My, POS: PRON, Lemma: my, Morph: Number=Sing|Person=1|Poss=Yes|PronType=Prs\n",
      "Text: friends, POS: NOUN, Lemma: friend, Morph: Number=Plur\n",
      "Text: and, POS: CCONJ, Lemma: and, Morph: ConjType=Cmp\n",
      "Text: I, POS: PRON, Lemma: I, Morph: Case=Nom|Number=Sing|Person=1|PronType=Prs\n",
      "Text: went, POS: VERB, Lemma: go, Morph: Tense=Past|VerbForm=Fin\n",
      "Text: to, POS: ADP, Lemma: to, Morph: \n",
      "Text: a, POS: DET, Lemma: a, Morph: Definite=Ind|PronType=Art\n",
      "Text: Fleshwater, POS: PROPN, Lemma: Fleshwater, Morph: Number=Sing\n",
      "Text: concert, POS: NOUN, Lemma: concert, Morph: Number=Sing\n",
      "Text: last, POS: ADJ, Lemma: last, Morph: Degree=Pos\n",
      "Text: night, POS: NOUN, Lemma: night, Morph: Number=Sing\n",
      "Text: in, POS: ADP, Lemma: in, Morph: \n",
      "Text: Chicago, POS: PROPN, Lemma: Chicago, Morph: Number=Sing\n",
      "Text: ., POS: PUNCT, Lemma: ., Morph: PunctType=Peri\n",
      "Entity: Fleshwater, Label: NORP\n",
      "Entity: last night, Label: TIME\n",
      "Entity: Chicago, Label: GPE\n"
     ]
    }
   ],
   "source": [
    "# 2. Experiment with changing words, adding punctuation, or introducing typos.\n",
    "\n",
    "#Changing words:\n",
    "text3 = (\"\"\"My friends and I went to a Fleshwater concert last night in Chicago.\"\"\")\n",
    "doc = nlp(text3)\n",
    "for token in doc:\n",
    "    print(f\"Text: {token.text}, POS: {token.pos_}, Lemma: {token.lemma_}, Morph: {token.morph}\")\n",
    "\n",
    "for ent in doc.ents:\n",
    "    print(f\"Entity: {ent.text}, Label: {ent.label_}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text: Wow, POS: INTJ, Lemma: wow, Morph: \n",
      "Text: !, POS: PUNCT, Lemma: !, Morph: PunctType=Peri\n",
      "Text: My, POS: PRON, Lemma: my, Morph: Number=Sing|Person=1|Poss=Yes|PronType=Prs\n",
      "Text: friends, POS: NOUN, Lemma: friend, Morph: Number=Plur\n",
      "Text: and, POS: CCONJ, Lemma: and, Morph: ConjType=Cmp\n",
      "Text: I, POS: PRON, Lemma: I, Morph: Case=Nom|Number=Sing|Person=1|PronType=Prs\n",
      "Text: went, POS: VERB, Lemma: go, Morph: Tense=Past|VerbForm=Fin\n",
      "Text: to, POS: ADP, Lemma: to, Morph: \n",
      "Text: a, POS: DET, Lemma: a, Morph: Definite=Ind|PronType=Art\n",
      "Text: Deftones, POS: PROPN, Lemma: Deftones, Morph: Number=Sing\n",
      "Text: concert, POS: NOUN, Lemma: concert, Morph: Number=Sing\n",
      "Text: this, POS: DET, Lemma: this, Morph: Number=Sing|PronType=Dem\n",
      "Text: weekend, POS: NOUN, Lemma: weekend, Morph: Number=Sing\n",
      "Text: in, POS: ADP, Lemma: in, Morph: \n",
      "Text: Indianapolis, POS: PROPN, Lemma: Indianapolis, Morph: Number=Sing\n",
      "Text: and, POS: CCONJ, Lemma: and, Morph: ConjType=Cmp\n",
      "Text: it, POS: PRON, Lemma: it, Morph: Case=Nom|Gender=Neut|Number=Sing|Person=3|PronType=Prs\n",
      "Text: was, POS: AUX, Lemma: be, Morph: Mood=Ind|Number=Sing|Person=3|Tense=Past|VerbForm=Fin\n",
      "Text: amazing, POS: ADJ, Lemma: amazing, Morph: Degree=Pos\n",
      "Text: !, POS: PUNCT, Lemma: !, Morph: PunctType=Peri\n",
      "Entity: Deftones, Label: ORG\n",
      "Entity: this weekend, Label: DATE\n",
      "Entity: Indianapolis, Label: GPE\n"
     ]
    }
   ],
   "source": [
    "#Adding punctuation:\n",
    "text3 = (\"\"\"Wow! My friends and I went to a Deftones concert this weekend in Indianapolis and it was amazing!\"\"\")\n",
    "doc = nlp(text3)\n",
    "#Token attributes and part of speech tagging:\n",
    "for token in doc:\n",
    "    print(f\"Text: {token.text}, POS: {token.pos_}, Lemma: {token.lemma_}, Morph: {token.morph}\")\n",
    "#Named entity recognition:\n",
    "for ent in doc.ents:\n",
    "    print(f\"Entity: {ent.text}, Label: {ent.label_}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text: My, POS: PRON, Lemma: my, Morph: Number=Sing|Person=1|Poss=Yes|PronType=Prs\n",
      "Text: friends, POS: NOUN, Lemma: friend, Morph: Number=Plur\n",
      "Text: and, POS: CCONJ, Lemma: and, Morph: ConjType=Cmp\n",
      "Text: I, POS: PRON, Lemma: I, Morph: Case=Nom|Number=Sing|Person=1|PronType=Prs\n",
      "Text: went, POS: VERB, Lemma: go, Morph: Tense=Past|VerbForm=Fin\n",
      "Text: to, POS: ADP, Lemma: to, Morph: \n",
      "Text: a, POS: DET, Lemma: a, Morph: Definite=Ind|PronType=Art\n",
      "Text: Deaftones, POS: PROPN, Lemma: Deaftones, Morph: Number=Sing\n",
      "Text: concert, POS: NOUN, Lemma: concert, Morph: Number=Sing\n",
      "Text: this, POS: DET, Lemma: this, Morph: Number=Sing|PronType=Dem\n",
      "Text: weeknd, POS: NOUN, Lemma: weeknd, Morph: Number=Sing\n",
      "Text: in, POS: ADP, Lemma: in, Morph: \n",
      "Text: Indiannapolis, POS: PROPN, Lemma: Indiannapolis, Morph: Number=Sing\n",
      "Text: ., POS: PUNCT, Lemma: ., Morph: PunctType=Peri\n",
      "Entity: Deaftones, Label: NORP\n",
      "Entity: Indiannapolis, Label: GPE\n"
     ]
    }
   ],
   "source": [
    "#Introducing Typos:\n",
    "text3 = (\"\"\"My friends and I went to a Deaftones concert this weeknd in Indiannapolis.\"\"\")\n",
    "doc = nlp(text3)\n",
    "for token in doc:\n",
    "    print(f\"Text: {token.text}, POS: {token.pos_}, Lemma: {token.lemma_}, Morph: {token.morph}\")\n",
    "\n",
    "for ent in doc.ents:\n",
    "    print(f\"Entity: {ent.text}, Label: {ent.label_}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### How does spaCy handle your modifications?\n",
    "#### Did any entities or tags change? Why might this happen?\n",
    "When I changed some words, spaCy recognized Fleshwater as a new entity, but not an organization as it did with the well-established band Deftones. It was instead recognized as a NORP, which stands for nationalities, or religious or political group. Fleshwater, of course, is none of these. It is a lesser-known, newer band. Chicago is still recignized as a GPE like Indianapolis was, and last night is still recognized as a time, but not a DATE like \"past weekend\" was. \n",
    "\n",
    "When I added \"!\" to the sentence, spaCy still treated the punctuation as a seperate entity and its entity recognition stayed the same. \n",
    "\n",
    "Finally, when I introduced typos in the words Deftones, weekend, and Indianapolis, spaCy still recognized each as its own entity, but failed to classify weekend as a date. \"Deaftones\" was then recognized as a NORP rather than a organization like before. \"Indiannapolis\" was still recognized as a GPE. \n",
    "\n",
    "These changes likely happened because spaCy relies on pre-trained data to recognize named entities, and so if a word is uncommon (like Fleshwater, for example, a lesser-known band name), it may not be labeled as we expect. Grammar, context, and surrounding words also affect how spaCy tags parts of speech and entities, which might explain the differences in recognition when typos were added."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
